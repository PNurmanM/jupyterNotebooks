# -*- coding: utf-8 -*-
"""mediaPipeHandTracking.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1baC43XQvcAOQYrGk_G5PSa0Khl05cET4
"""

!python --version

pip install mediapipe

pip install opencv-python

pip install opencv-contrib-python-headless

import mediapipe as mp
import cv2

import torch
print(torch.cuda.is_available())

!pip install opencv-contrib-python --upgrade

print("CUDA Available:", cv2.cuda.getCudaEnabledDeviceCount() > 0)

image = cv2.imread("1.jpg")
rgbImage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

from google.colab.patches import cv2_imshow as cvi

cvi(rgbImage)

mph = mp.solutions.hands
mpd = mp.solutions.drawing_utils

hands = mph.Hands(
    static_image_mode=False,
    max_num_hands=2,
    min_detection_confidence=0.5
)

results = hands.process(rgbImage)
hands.close()

resultsData = results.multi_hand_landmarks

for landmarkList in resultsData:
  for landmark in landmarkList.landmark:
    h, w, _ = image.shape
    x, y = int(landmark.x * w), int(landmark.y * h)

image.shape

for landmarkList in resultsData:
  mpd.draw_landmarks(image, landmarkList, mph.HAND_CONNECTIONS)

cvi(image)

import numpy as np

sequence = []
all_sequences = []
all_labels = []

#video = cv2.VideoCapture("/content/WIN_20250220_06_24_22_Pro.mp4")
video = cv2.VideoCapture("yes3.mp4")

hands = mph.Hands(
    static_image_mode=False,
    max_num_hands=2,
    min_detection_confidence=0.5
)

#for i in vidData:
  #cvi(i)


label = "yes"

while True:
  stat, vidData = video.read()

  if not stat:
    break

  RvidData = cv2.resize(vidData, (640,480))

  #cvi(vidData)

  vimage = cv2.cvtColor(RvidData, cv2.COLOR_BGR2RGB)
  vresult = hands.process(vimage)
  vdata = vresult.multi_hand_landmarks

  if vdata:
    for hand_landmark in vdata:
      landmarks = []
      for landmark in hand_landmark.landmark:
        landmarks.extend([landmark.x, landmark.y, landmark.z])

      sequence.append(landmarks)

      if len(sequence) > 30:
        sequence.pop(0)

      if len(sequence) == 30:
        all_sequences.append(sequence.copy())
        all_labels.append(label)

      #mpd.draw_landmarks(RvidData, hand_landmark, mph.HAND_CONNECTIONS)

  #cvi(RvidData)

x = np.array(all_sequences)
y = np.array(all_labels)

hands.close()

len(all_labels)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout

from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical

pip install joblib

import joblib
label_encoder = LabelEncoder()
y_fit = label_encoder.fit_transform(y)
joblib.dump(label_encoder, 'label_encoder.pk1')
y_fit

y_cat = to_categorical(y_fit)

x

y_cat

model = Sequential()
model.add(Bidirectional(LSTM(80, return_sequences=True, activation='tanh'), input_shape=(30, 63)))
model.add(Dropout(0.4))
model.add(Bidirectional(LSTM(40, return_sequences=False, activation='tanh')))
model.add(Dropout(0.4))
model.add(Dense(7, activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(x, y_cat, epochs=500, batch_size=4, validation_split=0.2)

model.save("sicthGestureModel.keras")

tvid = cv2.VideoCapture("/content/pleaseme.mp4")
tsequence = []

hands = mph.Hands(
    static_image_mode=False,
    max_num_hands=2,
    min_detection_confidence=0.5
)


while True:
  stat, vidData = tvid.read()

  if not stat:
    break

  RvidData = cv2.resize(vidData, (640,480))

  vimage = cv2.cvtColor(RvidData, cv2.COLOR_BGR2RGB)
  vresult = hands.process(vimage)
  vdata = vresult.multi_hand_landmarks

  if vdata:
    for hand_landmark in vdata:
      landmarks = []
      for landmark in hand_landmark.landmark:
        landmarks.extend([landmark.x, landmark.y, landmark.z])

      tsequence.append(landmarks)

      if len(tsequence) > 30:
        tsequence.pop(0)

      if len(sequence) == 30:
        x_test = np.expand_dims(tsequence, axis=0)
        prediction = model.predict(x_test)
        predicted_label = label_encoder.inverse_transform([np.argmax(prediction)])
        print(predicted_label[0])

      mpd.draw_landmarks(RvidData, hand_landmark, mph.HAND_CONNECTIONS)

  cvi(RvidData)

predicted_label

x_test

